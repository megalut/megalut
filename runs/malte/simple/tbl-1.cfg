# Configuration File for Tenbilac
# WARNING: ConfigParser configuration files are surprisingly weird for newbies:
# - Comment only on separate lines
# - Leave a field blank AND REMOVE THE ":" to specify "None"
# - But the latter only works if a string was expected, not for floats, ints, ...

[setup]

# Pick a name for your training run (runs with different settings can potentially be mixed later)
name: Tenbilac

# Path to a workdir where networks and results will be or are stored.
# MegaLUT uses its own workdir, taking precedence over this setting.
workdir: ./tenbilac-workdir

copyconfig: True

[norm]

# Which normer to use on the inputs, if any
oninputs: True
inputtype: sa1

# Same on the targets. If the activation function of your output nodes is linear (i.e., "iden"),
# and if you don't mind large weights (regul !), you usually don't need a target normer.
ontargets: False
targettype: -11

[net]

# Number of committee members
nmembers: 10

# The type of the networks (Net, MultNet)
type: MultNet
# Structure of hidden layers 
nhs: [3]

# For MultNets, specify here the intial weights for the additional nodes of the first layer.
# Set it to an empty list if you don't want the extra nodes.
mwlist: [(1, 1), (1, -1)]

# Names of the activation functions to use for different layers
actfctname: tanh
oactfctname: iden
multactfctname: iden

# Preset the network to transport the first of its inputs as output ?
startidentity: True
# Limit the number of nodes per layer which simply transport the outputs from the previous layer
# Set this to -1 if you don't want such a limit.
onlynidentity: -1

# Parameters controlling the noise added to the networks prior to training.
# Noise is only added to new blank networks, not if previously trained networks are reused.
addnoise: True
ininoisewscale: 0.3
ininoisebscale: 0.3
ininoisemultwscale: 0.1
ininoisemultbscale: 0.0


[train]

# Takeover (i.e., reuse) potential existing trainings or start the trainings from scratch ?
takeover: True

# The number of cores on which to run. Set it to 0 to run on all cores, and to 1 to avoid using multiprocessing.
ncpu: 10

# Name of the error function (mse, msb, ...).
errfctname: msb


# Regul is not yet fully implemented
useregul: False
#regulfctname
#regulweight

# Generic training parameters
valfrac: 0.5
shuffle: True
mbfrac: 0.3
#mbloops: 100
mbloops: 1


# Save plots ?
autoplot: True
# Plot (and track) the average prediction errors for each case ? Could be massive !
trackbiases: True
# Write the full status and history to disk at each iteration (not needed unless the system is unstable) ?
saveeachit: False
# Extra debug-logging at each call ?
verbose: False


# Choice of training algorithm
algo: multnetbfgs

# One of the following Sections is passed as kwargs to the selected algorithm.
# The fields are read through an eval(), therefore to pass a string use repr("hello").
[algo_multnetbfgs]

nepochs: 1
maxiter_sum: 25
maxiter_mult: 25

gtol: 1e-8


[algo_bfgs]
maxiter: 20
gtol: 1e-8


[predict]

# How to combine the committee results (mean, median)
combine: mean

